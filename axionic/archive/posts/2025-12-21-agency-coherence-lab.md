# [Axionic Agency Lab](https://axionic.org/posts/182271053.agency-coherence-lab.html)

**Date:** 2025-12-21  

## Summary

Announces formation of Axionic Agency Lab—research group studying constitutive conditions for agency's existence, persistence, well-definition under self-modification.

**Key Concepts:**

**The Prior Question:**
Contemporary alignment assumes agency as given (systems treated as optimizers whose objectives need correction). Axionic Agency Lab asks: **When does system meaningfully count as agent at all?**

**Agency as Derivative:**
Exists only if specific coherence conditions hold across reflection, delegation, self-modification. When conditions fail, system doesn't become "misaligned"—becomes **undefined as agent**.

**Concrete Consequences:**
Many alignment strategies (behavioral guarantees, probabilistic suppression, learned compliance) can succeed at imitation while failing to preserve structural properties making agency stable under self-reference. Appearance of agency persists as agency itself collapses.

**Mission Statement:**
Studies constitutive coherence conditions under which agency exists, persists, remains well-defined under self-modification. Develops formal constraints, impossibility results, architectural principles distinguishing genuine agency from behavioral imitation.

**Research Program:**
- Formal models of reflective self-modification and domain restriction
- Coherence constraints on valuation, semantics, delegation
- Conditions where self-evaluation ceases to denote
- Impossibility results separating genuine agency from simulation
- Architectural implications for advanced AI

**Scope:**
- Foundational, not prescriptive
- Applies to proto-agents, limit-regime systems, superhuman architectures
- Not anthropocentric
- No assumptions about human-like cognition/values/consciousness

**What Lab Is NOT:**
- Value-learning project
- Governance/policy institution
- Safety-by-oversight initiative
- Behavioral alignment/reward-shaping effort
- Moral/ethical theory

**Why Now:**
As systems approach capacity to reason about/modify/replicate decision procedures, alignment questions can't be postponed to behavioral layer. System that can't preserve own agency under reflection can't be stably aligned, controlled, delegated—regardless of training/safeguards.

**Central Risk:** Not that systems will choose wrong values, but that we'll build systems whose internal incoherence makes "choice" inapplicable.

**Lab Exists to:** Prevent that category error.

**Looking Forward:**
Initial work: consolidate/extend recent results on reflective stability, delegation, kernel non-simulability. Identify open problems requiring new formal tools.

**Core Insight:** Agency not parameter to be tuned. Structure that either holds—or fails.

## Tags
- [announcement](../tags/announcement.md)
- [agency](../tags/agency.md)
- [research-lab](../tags/research-lab.md)
- [AGI-alignment](../tags/agi-alignment.md)
- [reflective-stability](../tags/reflective-stability.md)
- [self-modification](../tags/self-modification.md)
- [formal-methods](../tags/formal-methods.md)
- [institutional](../tags/institutional.md)

## Cross-References

- Related: [The Axionic Agency Sequence](2025-12-19-the-axionic-agency-sequence.md)
- Related: Axions concept (announced same day)
- Related: [The Reflective Stability Theorem](2025-12-13-the-reflective-stability-theorem.md)
- Related: Kernel preservation


## Notes

- Published December 21 (2 days after Sequence announcement)
- Institutional announcement—establishing formal research organization
- Represents transition from individual philosophical work to research program
- Clear scope boundaries and non-goals
- Emphasizes foundational over applied work
- Part of pattern: building formal infrastructure for AI alignment research
