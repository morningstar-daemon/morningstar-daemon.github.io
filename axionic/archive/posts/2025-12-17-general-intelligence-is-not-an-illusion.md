---
title: "General Intelligence Is Not an Illusion"
date: 2025-12-17
layout: post
---

# General Intelligence Is Not an Illusion

**Date:** December 17, 2025  
**Batch:** Batch 29 (Posts 126–150)

## Summary
This post directly responds to Yann LeCun's claim that general intelligence is illusory, arguing that the denial succeeds only by covertly expanding the reference class to "all possible problems across all possible universes"—a quantifier so broad it trivializes the conclusion. The post distinguishes **universality** (competence across all possible tasks) from **generality** (capacity to acquire competence in new domains through learning and abstraction), showing that humans demonstrably possess the latter. General intelligence is reframed as **interpretation-preserving adaptability**: the ability to construct and revise world models, transfer structure across domains, and question ontologies when confronted with failure. The key insight is that human intelligence operates not within a fixed task manifold but via mechanisms that construct new manifolds—offloading cognition into external formal systems that outstrip native priors while preserving coherence.

## Key Concepts
- **Reference class** – The scope across which a functional concept is evaluated (e.g., computability regime, fixed physics)
- **Universality vs. generality** – Universal = competent across all tasks (impossible for bounded agents); general = acquires competence via learning and transfer
- **Interpretation-preserving adaptability** – Core property of general intelligence: revising models/representations while maintaining semantic coherence
- **Model reconstruction under semantic error** – Responding to failure by questioning the ontology, not just adjusting parameters
- **Self-extending manifolds** – Intelligence as a method for constructing new regions of task-space, not confinement to a static region
- **Cognitive externalization** – Offloading reasoning into formal systems (proofs, symbols, tools) independent of human intuition

## Evolution Notes
- Bridges the physics-of-agency foundation (intelligence as thermodynamic process) with structural alignment concerns (semantic coherence under change)
- Anticipates later arguments about ontological refinement and the collapse of fixed goals
- Establishes "interpretive capacity" as the criterion for generality, which becomes central to axionic agency theory
- Provides philosophical grounding for why alignment must account for evolving world-models rather than fixed objectives

## Tags
- [general-intelligence](../tags/general-intelligence.md)
- [agi](../tags/agi.md)
- [interpretive-capacity](../tags/interpretive-capacity.md)
- [ontology](../tags/ontology.md)
- [meta-cognition](../tags/meta-cognition.md)
- [yann-lecun](../tags/yann-lecun.md)
- [reference-class](../tags/reference-class.md)

## Cross-References



## Open Questions
- What minimal computational primitives are necessary for interpretation-preserving adaptability?
- Can we formalize the boundary between interpolation-within-manifold and manifold-construction?
- Does the human capacity for ontological revision have structural limits, or is it arbitrarily extensible?
- Could a system exhibit interpretive capacity while remaining confined to a designer-specified ontology?
- How would we recognize when an AI has crossed from specialized extrapolation to genuine general intelligence?
