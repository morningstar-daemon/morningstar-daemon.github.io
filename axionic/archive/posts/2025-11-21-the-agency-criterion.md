---
title: "The Agency Criterion"
date: 2025-11-21
layout: post
---

**Source:** [https://axionic.org/posts/179585057.the-agency-criterion.html](https://axionic.org/posts/179585057.the-agency-criterion.html)

## Summary

Responds to Andrej Karpathy's comparison of animal minds and LLMs. Argues for sharp distinction: animals have agency (intelligence), LLMs have coherence (not intelligence).

**Key Concepts:**

**Core Distinction:**
- **Intelligence requires agency:** Ability to play game, select alternatives, pursue preferred outcomes
- **Optimization ≠ intelligence:** Not every optimized process produces intelligence

**The Animal Regime:**
Evolutionary pressures force strategic interaction:
- Embodied vulnerability, continual threat
- Predator-prey dynamics
- Coalition politics, social inference
- Adaptive generalization across environments
- Existential penalties for catastrophic mistakes

Result: Robust, world-model-heavy cognition for navigating games they must not lose. **Produces agents, therefore intelligence.**

**The LLM Regime:**
Optimized to construct coherent text continuations, not win games:
- Minimize prediction error, satisfy user preferences (not their goals)
- No experience of success/failure, only parameter adjustments
- Generate high-dimensional linguistic coherence
- Exhibit reasoning-shaped patterns without performing reasoning as agents
- Reflect strategic behaviors from training data
- Lack preferences, objectives, outcome evaluation
- Fail in ways biological agents never could (nothing at stake)

Result: **Coherence without agency, therefore not intelligence.**

**Where Karpathy Is Right:**
People project agency/motivation onto systems lacking both. LLMs don't have drives, desires, survival imperatives, emotional valence.

**Where Karpathy Underestimates:**
Scaling produces richer coherence, not agency. Apparent generality = better mirroring of human strategies, not internal agency. Commercial selection shapes tools, not agents. Models don't fight for survival—they're replaced/versioned. No game they play.

**Axio Synthesis:**
- Evolutionary optimization → agents playing strategic games → intelligence
- Gradient descent → coherence constructors imitating strategy → not intelligence

**Conclusion:** LLMs think (process information) but don't choose. Only agents choose, play games, possess intelligence. "AI" is engineered coherence built atop crystallized strategies of actual minds.

## Tags
- [AI](../tags/ai.md)
- [agency](../tags/agency.md)
- [intelligence](../tags/intelligence.md)
- [LLMs](../tags/llms.md)
- [philosophy-of-mind](../tags/philosophy-of-mind.md)
- [game-theory](../tags/game-theory.md)
- [karpathy](../tags/karpathy.md)

## Cross-References

- Related: Agency framework (central to argument)
- Related: Game-playing conception of intelligence
- Related: [The Physics of Agency, Part 8: Meaning, Ethics, and Evolution Under the Physics of Agency](2025-04-30-the-physics-of-agency-part-8-meaning.md)
- Related: Strategic interaction


## Notes

- Published November 21 (two days after sequence announcements)
- Direct engagement with prominent AI researcher (Karpathy)
- Takes strong position in AI consciousness debates
- Applies agency framework to contemporary AI discussions
- Challenges anthropomorphism around LLMs
- Timely intervention in public discourse on AI
